### 语音入门知识：

* [极客时间](https://time.geekbang.org)

* [Labuladong 的算法笔记](https://labuladong.github.io/algo)

* [CTC Loss 原理](https://zhuanlan.zhihu.com/p/108547594)

* [十分钟读懂 Beam Search](https://zhuanlan.zhihu.com/p/114669778)

* [一文入门元学习 Meta-Learning（附代码）](https://zhuanlan.zhihu.com/p/136975128)

* [Transformer 通俗笔记：从 Word2Vec、Seq2Seq 逐步理解到 GPT、BERT](https://blog.csdn.net/v_JULY_v/article/details/127411638)

* [一文通透想颠覆 Transformer 的 Mamba：从 SSM、HiPPO、S4 到 Mamba](https://blog.csdn.net/v_JULY_v/article/details/134923301)

* [中文开源数据集汇总帖](https://github.com/wenet-e2e/wenet/issues/2097)

* [Librosa 语音信号处理](https://blog.csdn.net/qq_23981335/article/details/115753516)

* [一文弄懂 Pytorch 的 DataLoader, DataSet, Sampler 之间的关系](https://www.cnblogs.com/marsggbo/p/11308889.html)

### 语音自监督知识：

* [Wav2vec 系列：从原始音频中学习语音的结构 - 知乎](https://zhuanlan.zhihu.com/p/302463174)

* [Self-Supervised Learning for Pre-training on Speech Recognition 自监督预训练语音识别模型综述（一）](https://zhuanlan.zhihu.com/p/436905615)

* [Self-Supervised Learning for Pre-training on Speech Recognition 自监督预训练语音识别模型综述（二）](https://zhuanlan.zhihu.com/p/449154171)

* [自监督预训练（一） 语义部分](https://zhuanlan.zhihu.com/p/398845326)

* [自监督预训练（二） 语音部分](https://zhuanlan.zhihu.com/p/398846190)

* [自监督预训练（三）wav2vec 2.0原理](https://zhuanlan.zhihu.com/p/398849371)

* [Wav2vec2-一个基于自学习的语音表示方法 1 概览](https://zhuanlan.zhihu.com/p/531958632)

* [Wav2vec2-一个基于自学习的语音表示方法 2 模型构造](https://zhuanlan.zhihu.com/p/531995511)

* [Wav2vec2-一个基于自学习的语音表示方法 3 量化模块](https://zhuanlan.zhihu.com/p/532010094)

* [Wav2vec2-一个基于自学习的语音表示方法 4 model forward](https://zhuanlan.zhihu.com/p/532072093)

* [Wav2vec2 流式识别改造研究 - 知乎](https://zhuanlan.zhihu.com/p/467823705)

* [通用模型、全新框架，WavLM 语音预训练模型全解](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/122137936)

### 语音前沿动态：

* [低调奋进](https://yqli.tech)

* [SUPERB 官网](https://superbbenchmark.org)

* [ICASSP 2023 官网](https://ieeexplore.ieee.org/document/10096024)

* [INTERSPEECH 2023 官网](https://www.isca-archive.org/interspeech_2023/index.html)

* [ICASSP 2024 官网](https://ieeexplore.ieee.org/xpl/conhome/10445798/proceeding)

### Wenet 入门知识：

* [Github 开源地址](https://github.com/wenet-e2e/wenet.git)

* [Github 技术文档](https://wenet-e2e.github.io/wenet)

* [WeNet: Production Oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit](https://arxiv.org/pdf/2102.01547.pdf)

* [WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit](https://arxiv.org/pdf/2203.15455v1.pdf)

* [Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition](https://arxiv.org/pdf/2012.05481.pdf)

* [U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition](https://arxiv.org/pdf/2106.05642.pdf)

* [Wenet - 面向工业落地的 E2E 语音识别工具](https://zhuanlan.zhihu.com/p/349586567)

* [Wenet 网络设计与实现 1 - 端到端识别基础](https://zhuanlan.zhihu.com/p/381093937)

* [Wenet 网络设计与实现 2 - 网络结构](https://zhuanlan.zhihu.com/p/381095506)

* [Wenet 网络设计与实现 3 - Mask](https://zhuanlan.zhihu.com/p/381271607)

* [Wenet 网络设计与实现 4 - Cache](https://zhuanlan.zhihu.com/p/396703996)

* [动态 batch 和静态 batch 的原理和代码详解](https://blog.csdn.net/shaoyou223/article/details/122642487)

* [WeNet 更新：超大规模数据 UIO，支持千万小时语音训练任务](https://www.cnblogs.com/DataBaker/p/15774349.html)

### Espnet 入门知识：

# TODO

* [Github 开源地址](https://github.com/espnet/espnet)

* [Github 技术文档](https://espnet.github.io/espnet)

### Fairseq 入门知识：

* [Github 开源地址](https://github.com/facebookresearch/fairseq)

* [Github 技术文档](https://fairseq.readthedocs.io/en/latest)

* [基于判别学习的语音预训练模型（0）-- 简单总结](https://zhuanlan.zhihu.com/p/463864969)

* [基于判别学习的语音预训练模型（1）-- 从声学特征到自监督语音特征](https://zhuanlan.zhihu.com/p/463866895)

* [基于判别学习的语音预训练模型（2-1）-- CPC from DeepMind](https://zhuanlan.zhihu.com/p/463867673)

* [基于判别学习的语音预训练模型（3-1）-- wav2vec from FAIR](https://zhuanlan.zhihu.com/p/463868007)

* [基于判别学习的语音预训练模型（3-2）-- vq-wav2vec from FAIR](https://zhuanlan.zhihu.com/p/463868373)

* [基于判别学习的语音预训练模型（3-3）-- Discrete BERT from FAIR](https://zhuanlan.zhihu.com/p/463868745)

* [基于判别学习的语音预训练模型（3-4）-- wav2vec 2.0 from FAIR](https://zhuanlan.zhihu.com/p/463869002)

* [基于判别学习的语音预训练模型（3-5）-- wav2vec-U from FAIR](https://zhuanlan.zhihu.com/p/463869365)

* [基于判别学习的语音预训练模型（3-6）-- wav2vec 2.0 + ST from FAIR](https://zhuanlan.zhihu.com/p/465112281)

* [基于判别学习的语音预训练模型（3-7）-- HuBERT from FAIR](https://zhuanlan.zhihu.com/p/569958749)

* [基于判别学习的语音预训练模型（3-8）-- wav2vec-U 2.0 from Meta AI](https://zhuanlan.zhihu.com/p/570234555)

* [Fairseq 指南和源码解析](https://zhuanlan.zhihu.com/p/558760615)

* [Fairseq 的 ddp-backend 机制 —— 原理和实现](https://zhuanlan.zhihu.com/p/580852851)

### Whisper 入门知识：

# TODO

* [Hugging Face 复现 Whisper 文档](https://huggingface.co/docs/transformers/model_doc/whisper)

* [使用 🤗 Transformers 微调 Whisper 模型](https://huggingface.co/blog/zh/fine-tune-whisper)

* [FasterWhisper + WhisperX 本地语音识别使用方法](https://www.bilibili.com/read/cv26636573)

* [Whisper 的 C++ 代码实现](https://github.com/ggerganov/whisper.cpp)

* [Whisper 增加热词公众号介绍](https://mp.weixin.qq.com/s?__biz=MzkyNTI4NzI2OQ==&mid=2247484266&idx=1&sn=85fa6f71f27ed56f999cefd6b3ea2124&chksm=c1c99560f6be1c76947eee1dffdbaef219aef24b5e19bcaef41eb6a8b8734c2360256ee5678d#rd)

* [Whisper 增加热词原 issue](https://github.com/openai/whisper/pull/2070)

### FlashAttention 入门知识：

* [通透理解 FlashAttention 与 FlashAttention2：全面降低显存读写、加快计算速度](https://blog.csdn.net/v_JULY_v/article/details/133619540)

* [FlashAttention 图解（如何加速 Attention ）](https://zhuanlan.zhihu.com/p/626079753)

* [FlashAttention2 详解（性能比 FlashAttention 提升 200% ）](https://zhuanlan.zhihu.com/p/645376942)

* [FlashAttenion-V3: Flash Decoding 详解](https://zhuanlan.zhihu.com/p/661478232)

* [【 FlashAttention-V4, 非官方 】FlashDecoding + + ](https://zhuanlan.zhihu.com/p/665595287)

### AIGC 相关：

* [AIGC 系列文章汇总](https://developer.aliyun.com/article/1422750)

* [大模型时代下语音合成 NAR 模型及源码 —— SoundStorm](https://zhuanlan.zhihu.com/p/650647840)

* [虚拟女友初步调研总结 2024 - 0212](https://zhuanlan.zhihu.com/p/682012849)

* [Bert-vits2 音色迁移项目](https://mp.weixin.qq.com/s/iXbX5-AbzKNF2pf1sSQIEA)

* [SO-VITS-SVC 音色迁移项目](https://zhuanlan.zhihu.com/p/630115251)

* [GPT-SoVITS 音色迁移项目](https://www.bilibili.com/read/cv30898214)
